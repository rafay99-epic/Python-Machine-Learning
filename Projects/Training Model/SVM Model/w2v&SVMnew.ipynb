{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b508db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from wordsegment import load, segment, clean\n",
    "load() #loading segment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#nltk.download()\n",
    "import csv\n",
    "from csv import reader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "  \n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02714831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>ClassLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt use raya so lovely as a woman you shouldnt ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt user leewboydatscoldtygadwn bad for cuff in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt user r kind of brand dawg rt users baby lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt user g anderson use riva based she look lik...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt user he nika roberts the shit you hear abou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  ClassLabel\n",
       "0  rt use raya so lovely as a woman you shouldnt ...           2\n",
       "1  rt user leewboydatscoldtygadwn bad for cuff in...           1\n",
       "2  rt user r kind of brand dawg rt users baby lif...           1\n",
       "3  rt user g anderson use riva based she look lik...           1\n",
       "4  rt user he nika roberts the shit you hear abou...           1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('abc.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7d6a5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let's test our function  by writing this string\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll write a function which will clean the text and prepare it.\n",
    "def cleanText(text):\n",
    "    cleaned = re.sub(\"[^a-zA-Z0-9']\",\" \",text)\n",
    "    lowered = cleaned.lower()\n",
    "    return lowered.strip()\n",
    "\n",
    "cleanText(\"Let's test our function, by writing this string!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0bb534b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt use raya so lovely as a woman you shouldnt complain about cleaning up your house and as a man you should always take the trash out',\n",
       " 'rt user leewboydatscoldtygadwn bad for cuff in dat hoe in the st place',\n",
       " 'rt user r kind of brand dawg rt users baby life you ever fuck a bitch and she start to cry you be confused as shit',\n",
       " 'rt user g anderson use riva based she look like a tranny']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = np.asarray(data[\"tweets\"]),np.asarray(data[\"ClassLabel\"])\n",
    "\n",
    "x_cleaned = [cleanText(t) for t in x]\n",
    "x_cleaned[:4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc08ecd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also we should convert our categories to the integer labels\n",
    "label_map = {cat:index for index,cat in enumerate(np.unique(y))}\n",
    "y_prep = np.asarray([label_map[l] for l in y])\n",
    "\n",
    "label_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d05e6",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe7e3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'use',\n",
       " 'raya',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'as',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'you',\n",
       " 'shouldnt',\n",
       " 'complain',\n",
       " 'about',\n",
       " 'cleaning',\n",
       " 'up',\n",
       " 'your',\n",
       " 'house',\n",
       " 'and',\n",
       " 'as',\n",
       " 'a',\n",
       " 'man',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'take',\n",
       " 'the',\n",
       " 'trash',\n",
       " 'out']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_tokenized = [[w for w in sentence.split(\" \") if w != \"\"] for sentence in x_cleaned]\n",
    "x_tokenized[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd40887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process took 1.89 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Now we'll create our model \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = gensim.models.Word2Vec(x_tokenized,\n",
    "                 vector_size=100\n",
    "                 # Size is the length of our vector.\n",
    "                )\n",
    "\n",
    "end = round(time.time()-start,2)\n",
    "print(\"This process took\",end,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68c46c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('threw', 0.978693962097168),\n",
       " ('doe', 0.9782025218009949),\n",
       " ('lean', 0.9779654741287231),\n",
       " ('anyway', 0.9777856469154358),\n",
       " ('sooo', 0.9773495197296143),\n",
       " ('grade', 0.9772722125053406),\n",
       " ('general', 0.9770423769950867),\n",
       " ('lost', 0.9762409925460815),\n",
       " ('except', 0.9761629104614258),\n",
       " ('hmmm', 0.9760681390762329)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"hated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83706663",
   "metadata": {},
   "source": [
    "Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "538c19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequencer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 all_words,\n",
    "                 max_words,\n",
    "                 seq_len,\n",
    "                 embedding_matrix\n",
    "                ):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.embed_matrix = embedding_matrix\n",
    "        \"\"\"\n",
    "        temp_vocab = Vocab which has all the unique words\n",
    "        self.vocab = Our last vocab which has only most used N words.\n",
    "    \n",
    "        \"\"\"\n",
    "        temp_vocab = list(set(all_words))\n",
    "        self.vocab = []\n",
    "        self.word_cnts = {}\n",
    "        \"\"\"\n",
    "        Now we'll create a hash map (dict) which includes words and their occurencies\n",
    "        \"\"\"\n",
    "        for word in temp_vocab:\n",
    "            # 0 does not have a meaning, you can add the word to the list\n",
    "            # or something different.\n",
    "            count = len([0 for w in all_words if w == word])\n",
    "            self.word_cnts[word] = count\n",
    "            counts = list(self.word_cnts.values())\n",
    "            indexes = list(range(len(counts)))\n",
    "        \n",
    "        # Now we'll sort counts and while sorting them also will sort indexes.\n",
    "        # We'll use those indexes to find most used N word.\n",
    "        cnt = 0\n",
    "        while cnt + 1 != len(counts):\n",
    "            cnt = 0\n",
    "            for i in range(len(counts)-1):\n",
    "                if counts[i] < counts[i+1]:\n",
    "                    counts[i+1],counts[i] = counts[i],counts[i+1]\n",
    "                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n",
    "                else:\n",
    "                    cnt += 1\n",
    "        \n",
    "        for ind in indexes[:max_words]:\n",
    "            self.vocab.append(temp_vocab[ind])\n",
    "                    \n",
    "    def textToVector(self,text):\n",
    "        # First we need to split the text into its tokens and learn the length\n",
    "        # If length is shorter than the max len we'll add some spaces (100D vectors which has only zero values)\n",
    "        # If it's longer than the max len we'll trim from the end.\n",
    "        tokens = text.split()\n",
    "        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n",
    "        vec = []\n",
    "        for tok in tokens[:len_v]:\n",
    "            try:\n",
    "                vec.append(self.embed_matrix[tok])\n",
    "            except Exception as E:\n",
    "                pass\n",
    "        \n",
    "        last_pieces = self.seq_len - len(vec)\n",
    "        for i in range(last_pieces):\n",
    "            vec.append(np.zeros(100,))\n",
    "        \n",
    "        return np.asarray(vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a351b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],\n",
    "              max_words = 1200,\n",
    "              seq_len = 15,\n",
    "              embedding_matrix = model.wv\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0933b6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70528466,  1.21203542, -0.26196185, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec = sequencer.textToVector(\"you u got wild bitches tellin you lies\")\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4039bcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cb90e",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a89309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 1500)\n"
     ]
    }
   ],
   "source": [
    "# But before creating a PCA model using scikit-learn let's create\n",
    "# vectors for our each vector\n",
    "x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])\n",
    "print(x_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40431d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of variance ratios:  0.7418678952184677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(n_components=50)\n",
    "pca_model.fit(x_vecs)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbb632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_comps = pca_model.transform(x_vecs)\n",
    "x_comps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6f46068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 50)\n",
      "(4957, 50)\n",
      "(19826,)\n",
      "(4957,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_comps,y_prep,test_size=0.2,random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7fc4c",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "285ef726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classifier has fitted, this process took 58.02 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(x_train,y_train)\n",
    "\n",
    "end = time.time()\n",
    "process = round(end-start,2)\n",
    "print(\"Support Vector Machine Classifier has fitted, this process took {} seconds\".format(process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "597c264f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795844260641517"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc6c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
